742 The history of graphics cards can be traced back to the 1970s  when computer graphics were first being developed.  In the early days  graphics were generated by the computer's central processing unit (CPU)  which limited the quality and speed of graphics rendering. The first dedicated graphics card was developed in the late 1970s by IBM  which introduced the Monochrome Display Adapter (MDA) in 1981.  The MDA was a text-only display adapter that could display up to 80 columns of text on a monochrome monitor. In the early 1980s  the Color Graphics Adapter (CGA) was introduced  which was the first graphics card to support color graphics.  The CGA could display 16 colors at a resolution of 320x200 pixels. In the mid-1980s  the Enhanced Graphics Adapter (EGA) was introduced  which could display up to 16 colors at a higher resolution of 640x350 pixels. In the late 1980s and early 1990s  a number of new graphics standards were introduced  including Video Graphics Array (VGA)  Super VGA (SVGA)  and XGA.  These standards offered higher resolutions  more colors  and faster refresh rates  and helped to drive the development of dedicated graphics cards. In the mid-1990s  3D graphics technology began to emerge  with the introduction of the first 3D graphics accelerators.  These cards  such as the 3dfx Voodoo and the Nvidia RIVA  were designed to offload 3D rendering tasks from the CPU  resulting in smoother and more realistic graphics.  
