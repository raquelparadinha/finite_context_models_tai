89 The ever-growing World Wide Web consists of billions of linked HTML documents (and other resources)  but most of the links contain no information about why the linkage has been made or what it might mean.  Services such as Google can automatically trace the links and index each page   with the aid of â€œmetadataâ€ such as keywords that summarize page content.  However  discovering the relationships between data items on pages  or between pagesâ€”and their meaning  or semanticsâ€”requires human scrutiny. In his 1999 book Weaving the Web  World Wide Web creator Tim Berners-Lee   described a new way in which Web pages might be organized in the future:I have a dream for the Web [in which computers] become capable of analyzing all the data on the Webâ€”the content  links  and transactions between people and computers.  A â€œSemantic Web â€ which should make this possible  has yet to emerge  but when it does  the day-to-day mechanisms of trade  bureaucracy and our daily lives will be handled by machines talking to machines.  The â€œintelligent agentsâ€ people have touted for ages will finally materialize. In other words  by encoding definitions of objects and their relationships into the text of Web pages  programs   can be written to use this information to answer sophisticated questions such as â€œwhich devices from this vendor use open-source software?â€ 
